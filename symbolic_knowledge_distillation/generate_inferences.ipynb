{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Initial Imports and api key\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "### gpt3 function\n",
    "import openai\n",
    "import sys\n",
    "from inference_utils import scrub_PX_PY, name_PX_PY, get_key, gpt3 ,complete_gpt3, few_shot_prompt\n",
    "from interence_fs_templates import relation_dicts\n",
    "\n",
    "# define API key for GPT-3 access\n",
    "with open('api.key') as f:\n",
    "    openai.api_key = f.read().strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-belle",
   "metadata": {},
   "source": [
    "# Initialize Parameters and Results File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Figure out which heads we need to generate for\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# variable definitions\n",
    "batch_name = 'curie_inference_generation_0' ## define name of this batch\n",
    "engine = 'curie' # the GPT-3 engine to use\n",
    "top_p = 0.5# top_p for nucleus sampling\n",
    "n_gen = 10 # number of generations for each event/relation pair\n",
    "PersonX = 'Alex' # name to use for PersonX (to make sentences more natural)\n",
    "PersonY = 'Chris' # name to use for PersonY\n",
    "length = 12 # maximum token length of generations (number of words per generaation will be less)\n",
    "\n",
    "stop_token = '\\n\\n' # stop token during generation\n",
    "#d = HinderedBy_dict # which relation to useo\n",
    "n_examples_in = 5 # how many few-shot examples to use in the prompt\n",
    "\n",
    "# names to use in few-shot examples\n",
    "names = ['Jean','Robin','Charlie', 'Ryan','Taylor','Jordan','Riley','Jamie','Leslie','Rowan',\n",
    "               'Adrian','Ali','Wyatt', 'Sydney','Stevie','Shiloh', 'Sam','Pat','Noel','Nicky','Max',\n",
    "               'Madison', 'Lindsay','Leslie','Lee','Jesse','Hunter','Glen','Devin','Avery']\n",
    "\n",
    "\n",
    "\n",
    "heads_file = 'todo_events.txt'\n",
    "out_file = 'inference_gens.jsonl'\n",
    "meta_file = 'params.json'\n",
    "\n",
    "\n",
    "### save params if this is not done already (for records)\n",
    "if not os.path.isfile( meta_file):\n",
    "    relation_param_dict = [{key:d[key] for key in d.keys() if key !='function'} for d in  relation_dicts] \n",
    "    param_dict = {'batch_name':batch_name,\n",
    "                    'engine':engine,\n",
    "                    'top_p':top_p,\n",
    "                    'n_gen':n_gen,\n",
    "                    'PersonX':PersonX,\n",
    "                    'PersonY':PersonY,\n",
    "                    'n_examples_in':n_examples_in,\n",
    "                    'length':length,\n",
    "                  'stop_token':stop_token,\n",
    "                  'names':names,\n",
    "                 'relation_param_dict':relation_param_dict}\n",
    "    with open(meta_file, 'w') as f:\n",
    "        f.write(json.dumps(param_dict))\n",
    "        \n",
    "        \n",
    "### create output file if it does not exist\n",
    "if not os.path.isfile(out_file):\n",
    "    with open(out_file,'w') as f:\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-kazakhstan",
   "metadata": {},
   "source": [
    "# Generate Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Generated inferences for each event/relation pair\n",
    "\n",
    "'''\n",
    "import random\n",
    "import time\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "print_step = 400\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "# For robustness, figure out if we have already generated for some inputs, \n",
    "# (e.g. if the notebook crashed) and if so, pick up where we left off.\n",
    "####\n",
    "##\n",
    "## first, load the heads we need to generate for\n",
    "with open(heads_file) as f: \n",
    "    heads_all = [head.strip() for head in f.readlines()]\n",
    "## then, get all heads we have generated for so far\n",
    "heads_done = []\n",
    "with open(out_file) as f:\n",
    "    for line in f:\n",
    "        heads_done.append(json.loads(line)['head'])\n",
    "## heads todo are those in heads_file that aren't yet in out_file\n",
    "heads_todo = [head for head in heads_all if head not in heads_done]\n",
    "print('{} heads todo'.format(len(heads_todo)))\n",
    "##\n",
    "####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "# Next, begin generated on whichever inputs (event/relation) we still\n",
    "# need to generate for\n",
    "####\n",
    "\n",
    "for i in range(len(heads_todo)):\n",
    "    time.sleep(0.02)\n",
    "    \n",
    "    # get the event to generate for in this iteration\n",
    "    head_to_test = heads_todo[i]\n",
    "\n",
    "\n",
    "    ## first, define output objects (empty for now)\n",
    "    ex_result = {}\n",
    "    ex_result['head'] = head_to_test\n",
    "    ex_result['tails_by_relation'] = {}\n",
    "\n",
    "    ## randomize the names we use\n",
    "    random.shuffle(names)\n",
    "    names_list = list(zip(names[:15],names[15:]))\n",
    "\n",
    "    \n",
    "    \n",
    "    prompts = []\n",
    "\n",
    "    ## define inputs for GPT-3 (prompts) for each relation (defined by relation dict d)\n",
    "    for d in relation_dicts:\n",
    "        # get few-show examples\n",
    "        examples_in = d['examples'][:n_examples_in]\n",
    "\n",
    "        # define prompt, and name PersonX/PersonY in each example\n",
    "        prompt = few_shot_prompt(examples_in + [{'head':head_to_test}], d['function'], number=True,\n",
    "                                 Person_list=names_list[:len(examples_in)]+ [(PersonX,PersonY)])\n",
    "\n",
    "        # add prefix to prompt\n",
    "        prompt = d['prefix'] + prompt\n",
    "        prompt = name_PX_PY(prompt, PersonX,PersonY)\n",
    "\n",
    "        # add prompt to the output object\n",
    "        ex_result['tails_by_relation'][d['relation']] = {'relation':d['relation'],'prompt':prompt} \n",
    "        prompts.append(prompt)\n",
    "\n",
    "    ## generate using the prompt\n",
    "    result = complete_gpt3(prompts, length, engine, top_p = top_p,num_log_probs=1,n=n_gen, stop=stop_token,echo=False )\n",
    "\n",
    "\n",
    "    # for each prompt (corresponding to one event/relation input) generate inferences\n",
    "    j = 0\n",
    "    for d in relation_dicts:\n",
    "\n",
    "\n",
    "\n",
    "        ### sort the output\n",
    "        outputs = []\n",
    "        for choice in result['choices'][j:j+n_gen]:\n",
    "            out = choice['text']\n",
    "\n",
    "            # get the ind up to the stop_word, and take nll for this sequence\n",
    "            end_ind = choice['logprobs']['text_offset'].index( max(choice['logprobs']['text_offset']))\n",
    "            nll = sum(choice['logprobs']['token_logprobs'][:end_ind + 1])\n",
    "\n",
    "            outputs  += [{'text':out,\n",
    "                         'result':choice,\n",
    "                          'nll':nll}]\n",
    "        j+=n_gen\n",
    "        ex_result['tails_by_relation'][d['relation']]['tails'] = outputs\n",
    "\n",
    "    # append the generaitons to the output file\n",
    "    with open(out_file,'a') as f:\n",
    "        f.write(json.dumps(ex_result) + '\\n')\n",
    "        \n",
    "        \n",
    "    # print results\n",
    "    if (i % print_step) == 0:\n",
    "        print('='*50)\n",
    "        print('time: {} mins, avg_rate: {}'.format((time.time()-t_start)/60.,(time.time()-t_start)/60./(i+1) ))\n",
    "        print('{}) {}'.format(i, ex_result['head']))\n",
    "        \n",
    "        for relation in ex_result['tails_by_relation'].keys():\n",
    "            print('='*10)\n",
    "            print('{})'.format(relation))\n",
    "            print('='*10)\n",
    "            print('{}'.format([v['text'] for v in ex_result['tails_by_relation'][relation]['tails'] ] ))\n",
    "            \n",
    "        print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-physiology",
   "metadata": {},
   "source": [
    "# Process Generations once they are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "ONLY RUN once you have generated all inferences\n",
    "\n",
    "this block takes the full set of generated inferences,\n",
    "and puts them in a standardized format, including removing inferences\n",
    "that repeat for a given event/relation input.\n",
    "\n",
    "It also automatically divides generations into a train/val/test split\n",
    "based on the event (i.e. all inferences for a given event are sorted into the \n",
    "same split)\n",
    "\n",
    "generations are saved as a jsonl file where each entry has the following keys:\n",
    "\n",
    "head: the event for generation\n",
    "relation: relation for generation\n",
    "tail: the generated inference\n",
    "split: the dataset split\n",
    "\n",
    "'''\n",
    "\n",
    "import json\n",
    "    \n",
    "    \n",
    "full_dataset_file = 'unique_dataset.jsonl'\n",
    "    \n",
    "\n",
    "def name_PX_PY(s, PersonX, PersonY):\n",
    "    return s.replace('PersonX', PersonX).replace('PersonY', PersonY)\n",
    "def scrub_PX_PY(s, PersonX, PersonY):\n",
    "    return s.replace(PersonX, 'PersonX').replace(PersonY, 'PersonY')\n",
    "    \n",
    "def process_tail(tail):\n",
    "    if tail[-1] == '.':\n",
    "        tail = tail[:-1]\n",
    "    if tail[0] == ' ':\n",
    "        tail = tail[1:]\n",
    "    tail = scrub_PX_PY(tail, PersonX, PersonY)\n",
    "    \n",
    "    return tail\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import random\n",
    "i = 0\n",
    "\n",
    "data_out = []\n",
    "\n",
    "with open(out_file) as f, open(full_dataset_file,'w') as f_out:\n",
    "    for line in f:\n",
    "        \n",
    "        r = random.random()\n",
    "        if r< 0.8:\n",
    "            split = 'train'\n",
    "        elif r < 0.9:\n",
    "            split = 'val'\n",
    "        else:\n",
    "            split = 'test'\n",
    "\n",
    "\n",
    "        d = json.loads(line)\n",
    "\n",
    "        small_tails_by_relation = {}\n",
    "\n",
    "        for relation in d['tails_by_relation'].keys():\n",
    "            \n",
    "            # remove short inferences (degenerate)\n",
    "            d['tails_by_relation'][relation]['tails'] = [v for v in d['tails_by_relation'][relation]['tails'] if len(v['text']) > 2]\n",
    "            \n",
    "            small_tails_by_relation[relation] = {'relation':relation,\n",
    "                                                 'tails':[{'text':process_tail(v['text'])} for v in d['tails_by_relation'][relation]['tails']]}\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        for relation in small_tails_by_relation.keys():\n",
    "            inferences = [v['text'] for v in small_tails_by_relation[relation]['tails']]\n",
    "            \n",
    "            # do not include repeats\n",
    "            for inference in set(list(inferences)):\n",
    "                data_out.append({'split':split,'head':d['head'], 'relation':relation, 'inference':inference })\n",
    "        \n",
    "        new_d = {'split':split,'head':d['head'],'tails_by_relation':small_tails_by_relation}\n",
    "        \n",
    "    \n",
    "    for d in data_out:\n",
    "        f_out.write(json.dumps(d) + '\\n')\n",
    "\n",
    "        \n",
    "print('total of {} unique generated examples written to {}'.format(len(data_out), full_dataset_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-concentration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
